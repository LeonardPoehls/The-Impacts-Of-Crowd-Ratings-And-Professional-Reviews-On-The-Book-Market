
```{r 'check_ps', include=FALSE}

user.name = ''
```



# The Impacts of Professional Reviews and Crowd Ratings on the Book Market

Author: Leonard PÃ¶hls 

Hello readers and welcome to my Bachelor Thesis about how professional reviews and crowd ratings impacting the book market. Do not wonder about the manner of this Thesis. The whole Program is based on the statistical programming language R and its package RTutor to generate interactive problem sets with exercises for its readers. My created Problem is based on the Paper **Digitization and Pre-Purchase Information: The Causal and Welfare Impacts of Reviews and Crowd Ratings"** by Imke Reimers and Joel Waldfogel, that has been published in 2021 in the American Economic Journal. It investigates two main aspects. First, how pre-purchase information in the form of crowd ratings from other individual purchaser and professional reviews from daily newspapers do affect sales ranks and revenues. Second, to compare the welfare effect with and without the presence of pre-purchase information. The base of those mentioned welfare effects got determined through the transformation from sales prices and sales ranks into quantity elasticities. The Paper can be opened [here](https://www.aeaweb.org/articles?id=10.1257/aer.20200153). 

In 1995, one year after Amazon opened their gates, about more than 61% of all book sales in the USA has been generated by physical bookstores and bookclubs, while just 10 percent have been made from other channels including Amazon (Curcic, 2023). 

For the following 28 years we experienced the growth of digitization and development of crowd rating infrastructure on online pages. 
Hence, users got capable to receive important non-professional pre-information from other users that potentially influences purchasing behavior and economic welfare effects. As a result, online retailers substituted the trade of physical books in wide parts. From an economic perspective, pre-purchase information likewise effects personal expectation of quality and therefore also the demand.  

Today, the distribution of market share in print books have been changed significantly, while Amazon have taken over the leadership as the biggest retailer of print books in all over the world. For instance, the Amazon share for US book market accounts more than 40% percent and around 50% for the Uk market share (McLoughlin, 2022).

To examine these effects, this problem set aims to reorganize and replicate part of the study from Reimers and Waldfogel by retyping and extending their investigations about the effects of professional reviews and crowd ratings on sales ranks and revenues. 

## Exercise Content

1. Motivation

 1.1 Book Market, Professional Reviews and Crowd Ratings
 
 1.2 Introduction to Welfare, Demand and Price Elasticity

2. Data and Descriptive Insights

 2.1 Introduction to the Data Set
 
 2.2 Analysis of the pre-purchase information
 
 2.3 Recognition of Potential Effects through Descriptive Approaches
 
3. Empirical Strategies on Sales Ranks 

 3.1 Regressions, Robust Standard Errors and Fixed Effects 
 
 3.2 Estimation of the Effects on Sales Ranks and Prices
 
 3.3 Introduction and Implementation of Event Studies
 
4. Translating Sales Ranks into Quantities and Price Elasticity
 
5. Conclusion

6. References

Appendix - tbd.

A1. tbd.

A2. tbd.

A3. tbd.

## An Instruction how to work with Problem sets

As already mentioned above, the manner of this problem set is to create an interactive environment for its readers. Thus, sometimes appear different Types of exercises with a so called chunk (window) below. Basically, there are three different type of exercises:   
* An empty code chunk without any Information. Consequently, you have to find the solution by yourself. 
* Code chunks with gaps like ___ to replace with the correct code. 
* Those where the whole code is already given. This Code is ready to run.  
If you need some advice by solving the exercise, just press `hint` to get some help. By pressing `run` the code gets executed. The `solution` button is a short-cut to deliver the correct code immediately. To verify the task, click on `check`. If your code was not correct, you would get a corresponding report. 

Next to code chunks, you can also work on some multiple choice quizzes to test your prior knowledge or to check your own text comprehension. Guessing the right answer can also lead to a higher understanding and a maintaining attention. 

Press `Go to next exercise...` to continue and to find further instructive competitions.

<br/>


## Exercise 1 -- Motivation (3018 Words)

In this study, the book market was chosen to determine the impacts from crowd ratings and professional reviews. 

First of all, the following chapter intends to declare the meaning of "pre-purchase information", the book market situation and why especially this market is predestined for measuring review effects. To find an answer, we focus on newspaper magazines and on the role of Amazon concerning crowd ratings and so called sales ranks. 

In the second part of this chapter, we illustrate potential welfare effects by having access to pre-purchase information with and without the restriction of fixed book prices. Additionally, we explain the transition from sales ranks and sales prices into quantity elasticities.  

After editing this chapter, you gained an insight of the initial situation on the book market and you received deeper knowledge about basic economic issues. Thus you will be well prepared to continue with chapter 2. 

### Structure 

1.1 Book Market, Professional Reviews and Crowd Ratings

1.2 Introduction to Welfare, Demand and Price Elasticity


## Exercise 1.1 -- Book Market, Professional Reviews and Crowd Ratings

Generally, economists differentiate goods concerning their characteristics, their occurrences or other conditions. Focusing the degree of uncertainty, we distinguish between three different goods, **Search Goods**, **Experience Goods** and **Credence Goods**. Buyer of **Search Goods** already have an accurate perception and a high degree of certainty of what they want to buy. For instance, sugar or computer are typical search goods. **Experience Goods** are associated with a lower degree of certainty. Without any information advantages, buyers are not capable to assess the goods quality until the buyer starts comsuming it, such as visits to cinema or whine. By the consumption of **Credence Goods** the buyer never gets into the situation to evaluate the quality of the underlying good as the degree of uncertainty is the highest here. Common credence goods are services like lawyers or surgeons (Wieneke, 2019). 


Quiz: What would you guess books belong to?

[1]: Search Goods.
[2]: Experience Goods.
[3]: Credence Goods.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Books_As_Goods")
```

The underlying paper focuses the book market to examine the impacts from crowd ratings and professional reviews on sales ranks. So why is the book market particularly suitable for this study? In the underlying paper, Reimers and Waldfogel enumerated three main reasons for this claim. First, books belong to experience goods. For the other two goods, pre-purchase information is less or not relevant. Second, the number of professional reviews (in high visible media) is relatively small and distributed across a few big newspapers. The third reason refers to the data set on which the entire examination is based on. The high frequently data on book demand at Amazon should contain about 45% of the US physical book market, what is approximately comparable to Mcloughlins numbers from 2022. 

As explained, professional reviews are defined as periodically appearing reviews in daily newspaper articles. The data set includes information about the appearance of professional reviews from the New York Times, the Chicago Tribune, the Boston Globe, the Wall Street Journal, the Los Angeles Times and the Washington Post. However, quantitative information as well as star ratings are not available. Additionally, The New York Times recommends nine books every week (New York Times, 2023). Professional reviews enable access to pre-purchase information. 


Quiz: What is your suggestion, which of these newspapers has the most impact on the sales quantity?

[1]: The New York Times.
[2]: The Chicago Tribune.
[3]: The Boston Globe.
[4]: The Wall Street Journal.
[5]: The Los Angeles Times.
[6]: The Washington Post.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Big_Newspapers")
```

As well as professional reviews, the Amazon data set also includes information about star ratings from buyers. Identified as buyer on Amazon, everyone is permitted to evaluate the bought product on a five-point scale. Basically, people benefit from other customers reviews. However, in contrast to professional reviews, crowd rating are tending to generate less trust. Crowd ratings are susceptible to fake content for defaming or fraudulent purposes while professional reviews were created by objective and professional reviewers. Therefore, the more ratings exist for the particular product, the more likely the calculated average star rating approximates the "actual" quality. Thus crowd ratings are representing the second pre-purchase information.                                                                 
 
Hence, Reimers and Waldfogel claim that the consumer interacts with both of these types of pre-purchase information in a different way, perhaps resulting in different or superimposed effects. One possible reason could be the difference in accessibility of these information types. While crowd ratings are visible for every Amazon user, professional reviews are accessible but not automatically visible for everyone. Furthermore, it is less likely that people who randomly find a book on Amazon will verify the existence of a professional review afterwards. Vice versa, after finding a professional reviewed book, people automatically get access to crowd ratings during the purchasing process. Incidentally, word-of-mouth can be also understood as pre-purchase information. Nevertheless, the consumer has access to at least one type of pre-purchase information. 



Quiz: How many reviews on Amazon are fake or unreliable?

[1]: 61%.
[2]: 23%.
[3]: 9%.
[4]: 42%.
[5]: 15%.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Fakereviews_Amazon")
```


Previously, we mentioned to aim an estimation on so called sales ranks. Sales ranks are the numerical representation of how your products sells in contrast to other products in the same categorie (Wisniach, 2022). Actually, we do not have information about sold quantities so that sales ranks are substituting these values. [Amazon](https://www.amazon.com/gp/help/customer/display.html?nodeId=GGGMZK378RQPATDJ) defined their sales rank as hourly updated calculation to "reflect recent and historical sales of every item sold on Amazon". Hence, sales ranks are relative numbers to compare sales activities. To make assumptions regarding to a welfare analysis, the authors of the underlying paper collected more data from New York Times top-100 weekly bestsellers from 2018 to transform ranks into quantities. As a result, we are capable to examine price elasticities. 

**Summary**

To sum up, we obtained an explanation of goods concerning their level of certainty and assigned books to the experience goods. Furthermore, we enumerated three different types of pre-purchase information from which we use two for the estimation. We recognized that crowd ratings and professional reviews exhibit different effects and potentially superimpose each other what possibly leads to different results in estimation. Finally, we got an introduction to Amazons sales ranks and their importance for the further course. 

In the following chapter 1.2, you will pick up the economic basic knowledge you need to understand the analysis and the manner of this examination. 

## Exercise 1.2 -- Introduction to Welfare, Demand and Price Elasticity

Colloquially, the term "welfare" is associated with many contexts, such as unemployment benefit or other social assistance. The actual origin of this term lies in the economy. Mathematically, welfare is sum of producer surplus and consumer surplus. Basically, consumer surplus is the difference between the price for a good that consumers are willing to pay and the actual price of this good. Against this, Producer surplus is the difference between the price that suppliers would be willing to charge for their goods and the actual price of this good. Actually, the economic reality is much more complex. For simplifying purposes, we focus on polypol markets (markets with many suppliers) under perfect conditions, that the model requires in order to work. Without even one of these assumptions, the model is invalid.   


Quiz: Which of these conditions is **not** relevant for a perfect market?

[1]: Perfect information availability (Knowledge about every price for the underlying good).
[2]: No personal preferences (Preferences, that prevent you from acting rationally).
[3]: Homogenous goods (Equal goods).
[4]: Fast reaction velocity (Changing market conditions are quickly recognized from every market participant).
[5]: Every good has the same quality.
[6]: A large number of demanders and suppliers.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("perfectMarket_Conditions")
```

Under these conditions, we apply a graph to illustrate the situation between demand and supply to better understand the added value of pre-purchase information. 

**Task:** Run the following chunk to create this model. Press `check` to collect your points. 

```{r "4_1"}

#load the package "ggplot2"
library(ggplot2)
#Create a fictive data set 
demand <- c(2, 1.5, 1, 0.5, 0)
xAxis <- c(0, 1,2,3,4)
supply <- c(0, 0.5, 1, 1.5, 2)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Price = demand, Quantity = xAxis, Supply = supply, Group = xGroup)
#Create a plot
ggplot(data = DatasetTest) +
  geom_line(aes(x=Quantity, y=Price, group = Group), linetype = 1, size = 0.8) +
  geom_line(aes(x=Quantity, y=supply, group = Group), linetype = 1, size = 0.8) +
  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 0, y = 1, xend = 2, yend = 1), color = "red", linetype = "dashed") +
#Mark Zone A
  geom_text(aes(x=0.5, y=1.3, label = "A"), color = "black", hjust=0, size=8, alpha = 0.1) +
#Mark Zone B
  geom_text(aes(x=0.5, y=0.7, label = "B"), color = "black", hjust=0, size=8, alpha = 0.1) +
#Mark Zone C
  geom_text(aes(x=1.5, y=0.3, label = "C"), color = "black", hjust=0, size=8, alpha = 0.1) +  
#Mark intersection point
  geom_point(aes(x = 2, y=1), color = "red", size=2.5) +
#Add line description
geom_text(aes(x=0.5, y=1.83, label = "Demand"), color = "Black", angle = 315, size= 4, alpha=0.1) +
geom_text(aes(x=3.35, y=1.78, label = "Supply"), color = "Black", angle = 45, size= 4, alpha=0.1) +
#Set specific layout
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(labels = c("0", "", "Q*", "", "")) +
  scale_y_continuous(labels = c("0", "", "P*", "", "")) +
  theme(
 # axis.text.y=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())

```

The linear demand curve shows how the consumers act on a perfect market. Obviously, a maximum price exists at which no more is consumed. This model likewise assumes unrealistically that free goods are consumed infinitely. On the other side, the linear supply curve illustrates the suppliers point of view. The model concludes that every supplier can offer his good for the maximum price. Vice versa, the more the price falls, the fewer suppliers can still offer their good. In natural competition, suppliers are forced to align their prices. This is due to the fact that each supplier wants to maximize its turnover, so it is not worthwhile for the suppliers who can offer the lowest price to actually offer the lowest price. In long term, the actual market price will converge to P* and every supplier that can not offer for P* disappears. Finally, P* defines the equilibrium price and Q* the equilibrium quantity.  


Quiz: Which of these marked triangles represents the consumer surplus?

[1]: A.
[2]: B.
[3]: C.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Mark_Surplus")
```

**Price Elasticity**

As already mentioned, the previous model was a severe simplification of a complex market. For instance, the fiscus also impacts market activity by implementing price caps, by subsidizing various branches or in ensuring that no so called price cartels are created. Price cartels are an association of organizations closing price agreements for particular goods to bypass the competition. Usually, the course of the two curves is not linear and the conditions for a perfect market are not satisfied. The slope of the demand or supply curve depends on the so called **Price Elasticity**. For the demand side, **Price elasticity** indicates how demand reacts on changes in prices relatively. The formula looks as follows:  

$$
 \epsilon = \frac{\Delta Q/Q}{\Delta P/P} =  \frac{\%\,\mathrm{Change\,in\,the\,quantity\,of\,goods\,demanded}}{\%\,\mathrm{Change\,in\,price}}
$$

Hence, price elasticity delivers a value Îµ := [0, â] that can also show whether price increases would be worthwhile. An elasticity of Îµ = 1 tells us that a price increase of one percents implies a demand decrease of one percent and represents the maximum turnover point for suppliers. As a result, elasticities of Îµ < 1 are inelastic and otherwise elastic.   


Quiz: You are bar owner and want to maximize your turnover. In a particular period of time, you found out that the demand on beer is linear and a price increase from 3 Units to 4 Units implies a decreasing sales quantity from 1000 quantity units to 875 quantity units. Which of the following responses is correct?

[1]: The price is inelastic - You should not increase the price.
[2]: The price is elastic - You should increase the price by 47%.
[3]: The price is inelastic - You should increase the price by 34%.
[4]: The price is inelastic - You can easily more than double the price.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Elastic_orInelastic")
```

Price elasticity can be also shown graphically. The following graph illustrates the difference between elasticities in demand. 

**Task:** Execute the chunk below to see how the demand curve slope changes with different price elasticities

```{r "4_2",warning=FALSE}
#Create a fictive data set 
DatasetPE <- data.frame(
  Price = 1:10,
  Demand_05 = 10 - 0.5*1:10,
  Demand_1 = 10 - 1*1:10,
  Demand_2 = 10 - 2*1:10
)
colnames(DatasetPE) <- c("Price", "Îµ > 1", "Îµ = 1", "Îµ < 1")

DatasetPE <- DatasetPE %>% 
  pivot_longer(cols = starts_with("Îµ"), names_to = "Îµ", values_to = "Quantity_Value") 

# Umwandeln der Daten in "long format"
ggplot(data = DatasetPE, aes(x = Price, y = Quantity_Value, group = Îµ)) +
  geom_line() +
  theme_bw() +
  geom_segment(aes(x = 4, y = 8, xend = 4, yend = 9), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 9, xend = 2, yend = 9), color = "red", linetype = "dashed") +
  geom_text(aes(x=2.75, y=9.25, label = "ÎQU = 2"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=8.95, label = "ÎPU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=7.85, label = "Îµ = 2"), color = "black", angle = 337.5, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 6, xend = 4, yend = 7), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 7, xend = 3, yend = 7), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=7.2, label = "ÎQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=6.9, label = "PU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=5.45, label = "Îµ = 1"), color = "black", angle = 315, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 2, xend = 4, yend = 4), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 4, xend = 3, yend = 4), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=4.2, label = "ÎQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=3.45, label = "ÎPU = 2"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.6, y=1.7, label = "Îµ = 0,5"), color = "black", angle = 292.5, hjust=0, size=4, alpha = 0.1) +
  
  labs(title = "Demand Curves and their Price Elasticities",
       x = "Quantity in QU (Quantity Units)", 
       y = "Price in PU (Price Units)") +
  xlim(0, 10) + 
  ylim(0, 10) +
  scale_x_continuous(breaks = 0:10) +
  theme(panel.grid.minor=element_blank(),plot.background=element_blank()) 
```

Returning to the subject of pre-purchase information, a clear classification of these are needed. Alan T. Sorensen and Kenneth Train claimed that pre-purchase information changes the perception of the books quality and that a distinction between anticipated ex ante utility and experienced ex post utility is needed to measure the effect of these information on welfare. In simpler terms, we assume that a consumer obtains a different use depending on the availability of pre-purchase information. In sum, the consumer could face three various situations depending on the books expected quality. The consumer can expect lower quality than the actual quality ($\bar R_{j}$ < $R_{j}$), the same quality ($\bar R_{j}$ = $R_{j}$) or a higher expected quality than the actual quality ($\bar R_{j}$ > $R_{j}$). 

```{r "4_3",warning=FALSE}

#Create a fictive data set 
yPredQualityPrice <- c(1.6,1.0,0.50,0.15,-0.10)
xQuan <- c(1,2,3,4,5)
yRealQualityPrice <- c(2.1,1.5,1,0.65,0.4)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Pred_Price = yPredQualityPrice, Quan = xQuan, Real_Price = yRealQualityPrice, Group = xGroup)

#Create a plot
ggplot(data = DatasetTest) +
  geom_line(aes(x=Quan, y=Pred_Price, group = Group), linetype = 2, size = 0.9) +
  geom_line(aes(x=Quan, y=Real_Price, group = Group), linetype = 1, size = 0.9) +
  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 3, y = 0, xend = 3, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 0, xend = 4, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 0, y = 1, xend = 5, yend = 1),size=1.1, color = "red") +

#Mark Zone A
  geom_text(aes(x=1.25, y=1.15, label = "A"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1, xend = 1, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone B
  geom_text(aes(x=1.5, y=1.5, label = "B"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1.5, xend = 1, yend = 2.1), color = "black", linetype = "dashed") +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone C
  geom_text(aes(x=2.27, y=1.17, label = "C"), color = "black", hjust=0, size=5, alpha = 0.1) +
#Mark Zone D
  geom_text(aes(x=3.6, y=0.9, label = "D"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 4, y = 0.65, xend = 4, yend = 1), color = "black", linetype = "dashed") +
#Mark intersection points
  geom_point(aes(x = 2, y=1), color = "red", size=2.5) +
  geom_point(aes(x = 4, y=1), color = "red", size=2.5) +
  geom_text(aes(x=1.8, y=0.93, label = "P1"), color = "black", hjust=0, size=3, alpha = 0.1) +
  geom_text(aes(x=3.8, y=0.93, label = "P2"), color = "black", hjust=0, size=3, alpha = 0.1) +

#Add line description
  geom_text(aes(x=0, y=1.55, label = "Expected Quality without \nPre-Purchase Information\n _  _  _  _  _  _  _  _  _  _  "), color = "Black", hjust=0, size= 2.2, alpha=0.1) + 
  geom_text(aes(x=0, y=2.05, label = "Actual Quality with \nPre-Purchase Information\n ___________________"), color = "Black", hjust=0, size= 2.2, alpha=0.1) +
  
#Set specific layout
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(breaks = 1:5, labels = c("", "Q1", "Q*", "Q2", "")) +
  theme(
  axis.text.y=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())

```

As visualized in the upper graph, the consumer would choose quantity Q1 for $$(\bar R_{j} < R_{j})$$, quantity Q* for $$(\bar R_{j} = R_{j})$$ and quantity Q2 for $$(\bar R_{j} > R_{j})$$. For choosing Q1, the consumer would expect a surplus of triangle A while getting an actual surplus of A + B (ex post recognition of actual quality). A consumer with access to pre-purchase information would always choose Q* with a surplus of A + B + C. Overestimating the books quality, the consumer would buy quantity Q2 and obtains a surplus of A + B + C - D. It is obvious, that quality expectations affect the overall utility of the consumer and thus the demand curve appears to shift without impacting the price elasticity. In fact, the curve merely adjusts to the actual demand. 


Quiz: Which triangle represents the value added of pre-purchase information?

[1]: A
[2]: B
[3]: C
[4]: D

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("ValueAdded_Triangle")
```

**Summary**

After looking at a simple market diagram, we learnt to distinguish between demand and supply, to categorize the economic meaning of welfare and the composition of consumer / supplier surpluses and the formation of equilibrium values. Focusing the demand side, we got an insight into the topic of price elasticity to better understand the following examination of price elasticities on books. Hence, after collecting the necessary knowledge, we returned to pre-purchase information and their affects on welfare. We found out that pre-purchase information impacts the expectation for quality and leads to an adjustment of the demand curve. 

In the following chapter 2, we get an introduction to the Amazon data set followed by descriptive analyses.

## Exercise 2 -- Data and Descriptive Insights (3814)

The underlying examination by Reimers and Waldfogel is based on a data set provided by Amazon. 

In the first part, we deepen our understanding and the origin of the underlying data set and define important attributes that we later use for descriptive and empirical measurements.

Second, we focus on professional reviews and crowd ratings to investigate superficial contexts. To illustrate them, we use descriptive tables. 

Finally, we create an entire overview about descriptive analyses to detect potential effects between professional and non-professional reviews on sales ranks and prices to create a transition to the following empirical part of my Thesis.

After working through this chapter, you are surefooted in dealing with the underlying data set and thus well prepared to continue with the empirical part in chapter 3. 

### Structure

2.1 Introduction to the data set

2.2 Analysis of the pre-purchase information

2.3 Recognition of Potential Effects through overall Descriptive Approaches


## Exercise 2.1 -- Introduction to the Data Set

As already mentioned, the entire quantitative calculations are based on a data set provided by Amazon. The data set includes slightly less than 8.8 million observations of non-professional crowd ratings. One observation represents review(s) at one day between 02-01-2018 and 31-12-2018 for very book in every country. Observations were made in the USA, in UK and in Canada. In addition, the data set was merged with newspaper data from every newspaper magazine listed in 1.1 with information on whether the newspaper reviewed the particular book and whether the New York Times recommended the book. In fact, only 3.22 million observations were taken into consideration due to missing values, which reduces the actual market share what we examined. Reimers and Waldfogel claim that Amazon covered about 44.5 percent of the physical book market share in 2017, which we cannot accurately confirm. After removing about 63% of the data set, 44.5 percent cited by Reimers and Waldfogel means that only a little over 16 percent of the book market still covered by the Amazon data set. To improve the problem sets performance, we have already removed observations that are not relevant to our examination. 

To better understand our data set, we categorize some important variables and take a look at an excerpt of it. 

**Task:** Use the function `readRDS` to load the data set called `dataEst.RDS`. Save this data set under the name `data`.

```{r "6_1",warning=FALSE}
# Enter your code here.
#use readRDS("file_name.RDS") to load your data
```

You successfully loaded the data set. Now, we want to see how the data set is organized and which values the single attributes can take. 

**Task:** Use the function `head` to show the first rows from `data`. In addition, use the function `colnames` to list every column name from `data`.

```{r "6_2"}
#use head(data set name) to show your data set
```

As one can see, the main data set consists of 69 variables. Not each of these are essential for the following course and will not be considered further. For those to whom this does not apply will find an explanation below. 

**Identification Variables**

The variable `asin` represents the corporate amazon ID to differentiate between different products. `country` shows us whether the underlying review belongs to the US market (US), the Great Britain market (GB) or to the Canadian market (CA). `canum` is our main ID and combines these variables to create a powerful country-dependent identifier. A title-author identifier is stored in the variable `titleno`. Identification variables are indispensable for each data set to clearly distinguish each variable. In this data set, the combination of `ddate` (see below) and `canum` defines each unique observation. 

**Chronological variables**

`ddate` shows the main date on which the crowd ratings were created What all other chronological variables are based on. `NYT_elapse` (New York Times), `BG_elapse` (Boston Globe), `CHI_elapse` (Chicago Tribune), `LAT_elapse` (Los Angeles Times), `WAPO_elapse` (Washington Post), `WSJ_elapse` (Wall Street Journal) and `OTH_elapse` (All magazines except NYT) indicate how many days have passed since/until the publication of the individual professional review. As a reference date serves here `ddate`. `epos` (if > 0) and `eneg` (if < 0) are counting days since publication. 

**Value Variables** 

The variable `rank` stands for the Amazon sales rank, on the basis of which we will perform most of the calculations. `pamzn` shows us the price of the particular book while `R` provides the given star rating on a five-point scale. `review` delivers the number of reviews in total, so this variable delivers the same number for every specification of `canum`. Each of these attributes occurs twice and once each with an "l" in front of it. This means that the values are logarithmized. 

**Dummy Variables** 

Dummy variables are binary variables that can take only two values (here: 1 and 0). Any variable starting with `dnytpost` indicates "1" if the New York Times has published reviews within the period defined by the following numbers after `dnyt`. For instance, `dnytpost1_5` takes the value 1 if [0 <`NYT_elapse` < 6]. For `dnytpost` and `dnytpost10`, [0 <`NYT_elapse` < 10] and [11 <`NYT_elapse` < 20] holds. `dnytpostpre` takes the value 1 if [-10 < `NYT_elapse` < 20]. The same principle applies to all variables starting with `dothpost` where the variable `OTH_elapse` defines the base instead of `NYT_elapse`. 

To transform sales ranks from our main data set into quantities, the authors provided confidential data to determine quantities. This data got provided by Nielsen, a market research company, and Reimers and Waldfogel have published it only in already edited form. These mentioned confidential data sets contain information about weekly top 100 sold books between 2015 and 2018 while the accessible confidential data only delivers intermediate calculations to determine price elasticities. 

**Note:** In the embedded background of this environment in the further course again and again transformations of data are accomplished, without which many computations would not function. If significant changes of known variables or structures take place, these will be explained again in the course of the future tasks.

**Summary**

Apparently, with the underlying data set, we have not only a very large set of observations in a defined time period, but also a large amount of information in the form of variables. Especially in this case, where an already merged and edited dataset has been published, this large amount of information may also have a detrimental effect on the correct replication of this study. Nevertheless, we continue with the data set provided by Reimers and Waldfogel. In this chapter, we have gained insight into the data set by executing first code chunks in R and the key variables that will continue to guide us throughout the study. Finally, we learned how to define dummy variables and the importance of identifying variables. 

In chapter 2.2, we dare a first dive into the data set by creating descriptive statistics with focus on magazine reviews and Amazon crowd ratings. 


## Exercise 2.2 -- Analysis of the Pre-Purchase Information

In the following part, we will analyze data to gain a deeper understanding of the occurrence of professional ratings. In addition, we want to check whether the collected data from journals appropriates for following estimations. 

**Task:** Replace the ___ gaps in the following chunk to visualize the percentage of books reviewed by magazines, and then `check` it.

**Note:** In sum, the underlying data set includes data about **8770 books**. 

```{r "7_1",warning=FALSE}

#Creating two new dummy variables to indicate professional reviews from non-NYT Magazines and from all Magazines
data$DOTH <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 , 1, 0)
data$DALL <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 | data$DNYT == 1 , 1, 0)

#Calculate professional review shares 
Share_of_NYT_Ratings <- paste0(round(mean(data$DNYT, na.rm = TRUE)*100, 2), "%") 
Share_of_BG_Ratings <- paste0(round(mean(data$DBG, na.rm = TRUE)*100, 2), "%") 
Share_of_CHI_Ratings <- paste0(round(mean(data$DCHI, na.rm = TRUE)*100, 2), "%")
Share_of_LAT_Ratings <- paste0(round(mean(data$DLAT, na.rm = TRUE)*100, 2), "%") 
Share_of_WAPO_Ratings <- paste0(round(mean(data$DWAPO, na.rm = TRUE)*100, 2), "%") 
Share_of_DWSJ_Ratings <- paste0(round(mean(data$DWSJ, na.rm = TRUE)*100, 2), "%") 
Share_of_OTH_Ratings <- paste0(round(mean(data$DOTH, na.rm = TRUE)*100, 2), "%") 
Share_of_ALL_Ratings <- paste0(round(mean(data$DALL, na.rm = TRUE)*100, 2), "%")

#Create a table to illustrate the shares
DesRat <- data.frame(Share_of_NYT_Ratings = c(Share_of_NYT_Ratings, n_distinct(data$titleno[data$DNYT == 1])), Share_of_BG_Ratings = c(Share_of_BG_Ratings, n_distinct(data$titleno[data$DBG == 1])), Share_of_CHI_Ratings = c(Share_of_CHI_Ratings, n_distinct(data$titleno[data$DCHI == 1])), Share_of_LAT_Ratings = c(Share_of_LAT_Ratings, n_distinct(data$titleno[data$DLAT == 1])), Share_of_WAPO_Ratings = c(Share_of_WAPO_Ratings, n_distinct(data$titleno[data$DWAPO == 1])), Share_of_DWSJ_Ratings = c(Share_of_DWSJ_Ratings, n_distinct(data$titleno[data$DWSJ == 1])), Share_of_OTH_Ratings = c(Share_of_OTH_Ratings, n_distinct(data$titleno[data$DOTH == 1])), Share_of_ALL_Ratings = c(Share_of_ALL_Ratings, n_distinct(data$titleno[data$DALL == 1], n_distinct(data$titleno))))

#Create Row Names
rownames(DesRat) <- c("Relative share", "Absolut share") 

#Use of kbl() funktion to create a visualable table
DesRat  %>%
kbl(col.names = c("Share_of_NYT_Ratings" = "New York Times", "Share_of_BG_Ratings" = "Boston Globe", "Share_of_CHI_Ratings" = "Chicago Tribune", "Share_of_LAT_Ratings" = "Los Angeles Times", "Share_of_WAPO_Ratings" = "Washington Post", "Share_of_DWSJ_Ratings" = "Wall Street Journal", "Share_of_OTH_Ratings" = "Non New York Times", "Share_of_ALL_Ratings" = "All"), caption = "Share of Professional Reviewed Books ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 
```

In total, 12.66 percent of all books (1521) included in the data set have been professionally reviewed. 11 percent (1315) of these books were reviewed by the New York Times, representing the largest percentage of books reviewed by professionals. The second largest percentage provides the Chicago Tribune with only 1.36 percent (139), which is approximately ten times less than the number of reviews published by the New York Times. Overall, non-New York Times magazines account for nearly 3.1 percent of the share. The following calculations distinguish between The New York Times and non-New York Times magazines, so we should note the size of the intersection between these distinctions. 

**Task:** Run the following chunk to determine the intersections between the single magazines. 

```{r "7_2",warning=FALSE}

#gt_table

#Create a table to illustrate the shares
DesIntersection <- data.frame(Intersection_NYT = c(NA, n_distinct(data$titleno[data$DNYT == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DWSJ == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DOTH== 0])),
  
                              Intersection_LAT = c(n_distinct(data$titleno[data$DLAT == 1 & data$DNYT == 1]), NA, n_distinct(data$titleno[data$DLAT == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DWSJ == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DWSJ == 0 & data$DWAPO == 0 & data$DCHI == 0 & data$DBG == 0 & data$DNYT == 0])),
                              
                              
                              Intersection_BG = c(n_distinct(data$titleno[data$DBG == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DBG == 1 & data$DLAT == 1]),  NA, n_distinct(data$titleno[data$DBG == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DBG == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DBG == 1 & data$DWSJ == 1]),n_distinct(data$titleno[data$DBG == 1 & data$DWSJ == 0 & data$DWAPO == 0 & data$DCHI == 0 & data$DLAT == 0 & data$DNYT == 0])),
                              
                              Intersection_CHI = c(n_distinct(data$titleno[data$DCHI == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DCHI == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DCHI == 1 & data$DBG == 1]), NA, n_distinct(data$titleno[data$DCHI == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DCHI == 1 & data$DWSJ == 1]),n_distinct(data$titleno[data$DCHI == 1 & data$DWSJ == 0 & data$DWAPO == 0 & data$DLAT== 0 & data$DBG == 0 & data$DNYT == 0])),
                              
                              Intersection_WAPO = c(n_distinct(data$titleno[data$DWAPO == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DCHI == 1]), NA, n_distinct(data$titleno[data$DWAPO == 1 & data$DWSJ == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DWSJ == 0 & data$DLAT == 0 & data$DCHI == 0 & data$DBG == 0 & data$DNYT == 0])),
                              
                              Intersection_DWSJ = c(n_distinct(data$titleno[data$DWSJ == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DWAPO == 1]), NA, n_distinct(data$titleno[data$DWSJ == 1 & data$DWAPO == 0 & data$DLAT == 0 & data$DCHI == 0 & data$DBG == 0 & data$DNYT == 0])))
                    
#DesIntersection <- rbind(DesIntersection, colSums(DesIntersection, na.rm = TRUE))

row.names(DesIntersection) <- c("New York Times", "Los Angeles Times", "Boston Globe", "Chicago Tribune", "Washington Post", "Wall Street Journal", "Without Intersections")

DesIntersection  %>%
kbl(col.names = c("Intersection_NYT" = "â© New York Times", "Intersection_LAT" = "â© Los Angeles Times", "Intersection_BG" = "â© Boston Globe",  "â© Intersection_CHI" = "â© Chicago Tribune", "â© Intersection_WAPO" = "â© Washington Post", "Intersection_DWSJ" = "â© Wall Street Journal"), caption = "Intersections between New York Times and other magazines ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 

```

Looking at the New York Times reviews, we noticed that a number of 201 books were reviewed by at least one other magazine. Conversely, a larger proportion of the reviews from other magazines got were also rated by the New York Times. To determine the exact proportion, we calculated that at least 30 percent (Wall Street Journal) and at most 67 percent (Boston Globe) of non-New York Times magazines were also reviewed by the New York Times. As a result, estimates in this regard may be biased by mutual overlays. The row or column sum does not reflect the total number of reviews in the journals, as we only consider individual overlays. In reality, there are also multiple overlays where three or four journals review one book. 

The advantage for crowd ratings is that we also have the amount of ratings available. In fact, the selection of books to be evaluated by professional reviewers is not random (Berger, Sorensen, Rasmussen, 2010). We want to review whether book ratings from professional magazines are randomly distributed on their five-points scale on Amazon. If this is not the case, we cannot reject the assumption that journals only review "better" rated books on Amazon and vice versa. The left Graphs in the following illustrates that the distribution of professional reviews among crowd ratings is evenly distributed. 

On the other hand, whether authors are popular also depends on the number of books they have published so far. If the authors name is more popular through previous published books, professional reviewers could potentially tend to prefer those authors. The chart on the right visualizes the proportion of professional reviews among the number of the books previously published by the authors. 



Quiz: Focusing the published books, What is your suggestion?

[1]: The more books an author publishes, the more likely these books will be professionally reviewed.
[2]: The more books an author publishes, the less likely these books will be professionally reviewed.
[3]: The overall distribution is even.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Distribution_Guess")
```

**Task:** Replace every ____ gap with ....... and check this chunk to create the data set the following graphs are based on. 
```{r "7_3",warning=FALSE}
# Enter your code here.
```

After we have created the dataset, we can proceed with the creation of the chart.

**Task:** Execute the following chunk to create two graphs representing the distribution of professional reviews among two variables. 

```{r "7_4",warning=FALSE}
# Enter your code here.
```

If we focus on the distribution of Amazon star ratings, we recognize a slight leftward skew in the weaker rated books on Amazon. However, this high proportion is to the left of the 25 percent quantile line which states that 75 percent of ratings on Amazon are rated 4.1 stars or even better. In the 75 percent zone, the distribution is rather even while outliers in the 25 percent zone are not unusual. This could be because books that have already been rated poorly on Amazon tend to be rated less often as a result. Hence, we can assume that Amazon star ratings do not visibly influence the book selection of professional reviewers and vice versa. Actually, a remaining risk always exists. 

The on right chart, we notice a quite even distribution among the number of published books between 0 and 21 percent. The 25 percent quantile line is located at two published books which states that a huge amount of 25 percent of all books reviewed on Amazon are written by "inexperienced" authors who published two books or less. The distribution remains even until 65 published books. After that, almost no more books were reviewed. Because of that, we limited the x-axis. Similar to star ratings, we can assume that the book selection of professional reviewers is less dependent on the number of books published so far. 

**Summary**

In summary, we have gained deeper insights into the relative and absolut numbers of professional reviews. It was illustrated to us that a relatively high proportion, between 30 and 67 percent, of all non-New York Times magazines were also reviewed by the New York Times, which may bias future estimates. To disprove the assumption that the selection of professionally reviewed books is not random, we use two different variables (Amazon Star Rating and Number of previous published books) to check whether the relative distribution on said variables disproves this assumption. In both cases we found out, that the distributions regarding to these variables are more or less even which is why we may assume a random choice of books for the time being. After all, the professional reviewers at the New York Times have discretion over about which books they actually review. Accordingly, we can never assume a 100 percent random distribution.

## Exercise 2.3 -- Recognition of Potential Effects through Descriptive Approaches

Before we start identifying potential effect, we need a good overview of the relevant descriptive values. Generally, descriptive values are based on simple statistics to describe and to visualize contexts, that have already happened. These values are easily understood and can provide sound summaries of high data volumes within seconds. The most powerful function of descriptive analyses is the ability to identify relationships in order to make predictions using more complex statistics. This procedure is exactly the same as what we have already done in chapter 2.2. 

Starting with a good overview, we want to create a table including simple descriptive values distinguished by `country`. 

**Task:** Try to fill any ____ gap with ....... and check this chunk to create an overview of the main descriptive values.

```{r "8_1",warning=FALSE}
# Enter your code here.
```


The first four rows return the mean values of the book price, the Amazon Star Rating, the sales rank and the number of ratings per book. The book markets in all three considered countries do not have fixed book prices. Fixed book prices (FBP) means that the publisher has the exclusive right to set the price of his book. The retailer is not permitted to discount more than five percent from this set price (Nakayama, 2015).


Quiz: Comparing UK (without FBP) and Germany (with FBP), which country accounted a higher price increase between 1996 (end of FBP in UK) and 2018?

[1]: United Kingdom (UK).
[2]: Germany.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Fixed_Prices")
```

Unfortunately, our data set only includes countries data from countries without FBP. Comparing the prices of the three available countries, we recognize a large price difference between Canada (21.07) and Great Britain (13.12) while the US account an average price of 15.86. Possible reasons for those expensive book prices could be that Canada imports many of these books where fees and other costs are incurred (Kwan, 2013), transportation costs over large land masses and a loss of economies of scale due to a smaller book market. The star ratings and their percentiles are quite even while large differences occur in the sales ranks and the number of ratings. Considering the fact that sales ranks are generated on their individual market place and less transparent, we cannot list any specific reasons for this. The high differences in the number of ratings may be due to the level of awareness of Amazon in the individual countries. Obviously, the US market accounts for more than twice as many observations as Canada or Great Britain.      

Hence, we gained insight into general descriptive values over the entire data set. To elaborate this, let us review the same values for observations where the availability of professional reviews is guaranteed. 

**Task:** Use the fuction `readRDS` to read the RDS-file `DataDescriptiveJournals`. Save this data set under the file name `DataDescriptiveJournals`. In Addition, create a table with `kbl()` according to the table above.

```{r "8_2",warning=FALSE}
# Enter your code here.
```

Without differentiating between magazines or knowing whether the review turned out "positive" or "negative", we find an increase of price of about 15 percent, a decrease in average sales rank of about 39 percent, and the average number of ratings of about 86 percent, as well as significantly higher variance in percentiles of star ratings. The Amazon star rating also drops by a smaller percentage of about 2.3 percent. This leads to the assumption that the occurrence of professional reviews extensively affects the price, the sales rank and the number of ratings. 

The variance in percentiles of star ratings could be due to the fact that we have no information about how it has been reviewed, with which a polarization of the reviews may have taken place. For the New York Times reviews, the main data set contains information about whether the New York Times recommended a book using the variable `drecommended`. In the following, we generate two chronological graphs to show the descriptive affects from professional reviews on sales ranks and prices. 

**Note:** For further examinations we distinguish between the US data and the entire data, as the US market is the largest and has the most average crowd ratings (cno == 3 -> US). 

**Task:** Execute the following chunk to create three different filtered data sets. Press `check` to collect your points.

```{r "8_3",warning=FALSE}
# Enter your code here.
```
 
After creating these three datasets, proceed to transform the data by creating the final dataset to visualize the results. 
 
**Task:** Replace every ____ gap with the correct code and check this chunk to create the data set the following graphs are based on. 
 
```{r "8_4",warning=FALSE}
# Enter your code here.
```

The Price differences between New York Times recommended and non-recommended books is not significant while there is a big price gap between New York Times data and non new York Times data. The fact that the price difference between recommended and non-recommended books is quite small could be attributable to the fact that a non-recommendation does not always equate to a poor rating. On the other hand, books not mentioned in the New York Times are on average about $2.8 US cheaper than books mentioned in the New York Times.                    Observing the average sales ranks, we determine clearer differences between these three categories. Since the sales ranks are not direct quantity data, they can still serve as a quite useful comparative value. We see a decline of approximately 35% in sales rank from category to category. 


Quiz: Which of the following statements may be made?

[1]: With the information of these graphs we can assume a price elasticity Îµ < 1 for the US book market.
[2]: None of those listed.
[3]: With the information of these graphs we can assume a price elasticity Îµ > 1 for the US book market.
[4]: The effect of professional reviews on sales ranks (quantities) could be higher than the effect on prices.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Prices_and_SalesRanks")
```

We want to conduct a similar examination for crowd ratings, with the difference that we have information not only on the occurrence of these ratings, but also on their level on the five-point scale. Above, we already noted a negative association between the Amazon star rating and the occurrence of professional reviews. 

Before we start, let us divide Amazon star ratings into three categories: less than three stars, three and four stars, and more than four stars. 

**Task:** Create three dummy variables that show `1` when the star rating is between one and three, three and four and more than five. Orientate on exercise `2.2.1` to create a dummy variable. Then, try to replace the ___ gaps to create three filtered data sets according to `2.3.3`. Name these data sets `dataCR1_3`, `dataCR3_4` and `dataCR4_5`.


```{r "8_5",warning=FALSE}
# Enter your code here.
```

**Task:** Now run the following chunk to see how crowd ratings affect the price and sales ranks. Press `check` to confirm. 

```{r "8_6",warning=FALSE}
# Enter your code here.
```

In contrast to the affects of professional reviews, the mean price exhibits an inverse behavior. The Price seems to decrease the more stars a rating has while sales ranks generally behave according to the occurrence of professional reviews. Obviously, crowd ratings affect prices differently than professional reviews in magazines. This could be due to professional reviews triggering a (short-term) increase in demand that may not be served in the short term, leading to a price increase. On the other hand, higher crowd ratings on Amazon could rather resulting in long-term increase in demand, which leads to more contested competition, where everyone has to reduce their prices.                                                                 In fact, book pricing depends on more factors than are included in our data set. 

The mean sales rank decreases the more stars were given. As we noticed, more than 75 percent of all observations were rated with four stars or higher, so we can assume a similar percentage of observations in the upper bar (565063). Nevertheless, the mean sales rank for books recommended by the New York Times is significantly lower than the mean sales rank of books rated more than four stars by the crowd. Nevertheless, the mean sales rank for New York Times-recommended books is significantly lower than the mean sales rank for crowd ratings rated higher than four stars. 

To verify whether absolut book prices remain constant over time, we create a timeline for the average book price. We only observe for top 10000 and top 1000 sales ranks because we claim higher time based dependencies the 

**task:** Replace every ___ gap with the correct code. Use `left_join` to merge the data sets `dataTop10000` and `dataTop1000` by `ddate`

```{r "8_7",warning=FALSE}
# Enter your code here.
```

Overall, prices rise within one year by approximately $1.5 US. Price collapses can be observed in the spring, and there is a massive drop in prices in the summer months from June to September, with average prices (Top 1.000) falling by around 14 percent. The slump at the end of November is more like an outlier value. Obviously, better sold books tend to vary in price due to specific seasons. Lower prices in summer could be due to lower demand, as people prefer to read during the winter months. For the following event study in chapter 3.3 we have to take into account that there are fluctuations in book prices within a year and that these can be attributed to various factors. 

**Summary**

Using descriptive analysis, we gained deeper insight into the data and were able to establish initial associations between variables related to pre-purchase information and price or quantity (sales ranks). First, we created an overview over general values of the underlying data set. By selectively filtering out certain observations that were effected by pre-purchase information from professional reviewers, we found differences primarily in prices and sales ranks.                                                            We have deepened our research and found out that both professional reviews and Amazon crowd ratings negatively affect the sales rank (a lower rank implies higher volumes sold). Conversely, crowd ratings lower average prices while professional reviews raise average prices. Finally, we reviewed whether book prices fluctuating within one year and found out that the more "successful" books are, the more the price fluctuates within one year. For long-term studies, we need data that go beyond a one-year horizon. 

In the following chapter, we start with the empirical part of this problem set. 

## Exercise 3 -- Empirical Strategies on Sales Ranks and Prices

In following chapter *Empirical Strategies on Sales Ranks and Prices* we focus on empirical methods in general and their application to real data from Amazon. 

First, we give an introduction to empirical basics about the subject of regressions and the specific methods that come along with it. In general, it deals with control variables, fixed effects, robust standard errors, and the use of logarithmic estimates. 

In chapter 3.2, we focus on the replication of the main regression originally created by Reimers and Waldfogel. We discuss different of those effects and deepen the examination with further regressions. Based on these regressions on real data, we also focus on the validity of regressions and the explanation of values that inform them. 

Chapter 3.3 explains the subject of event studies. We also conduct two event studies based on the underlying data set to identify long- and short-term reactions of sales ranks and prices to professional reviews. 

Working through this chapter will provide fundamentals to advanced empirical strategies that are used to make predictive statements in economic science. 

### Structure

3.1 Regressions, Robust Standard Errors and Fixed Effects 
 
3.2 Estimation of the Effects on Sales Ranks and Prices
 
3.3 Introduction and Implementation of Event Studies

## Exercise 3.1 -- Regressions, Robust Standard Errors and Fixed Effects

Before we get into the subject matter of regressions, we learn how to classify this topic. In general, regressions describe a quantitative statistical approach to explaining associations between a dependent variable and one (linear) or more (multiple) independent (explanatory) variables in order to gain predictive information. The formula for a linear regression is as follows:

$$ y = x_0 + Î²_1x_1 + \varepsilon $$

where y indicates the dependent variable and x1 represents the explanation variable. x0 denotes the so called intercept on the y-axis, id est the average value of y if the explanatory variable x1 has no influence. Îµ is the error term and shows the sum of the residuals from the regression. Finally, Î²1 indicates the average increase or decrease of y for every unit increase of x1. Here, the value of Î²1 is determined by the Ordinary least squares method (OLS). Further information about this method you find [here](https://www.mathsisfun.com/data/least-squares-regression.html).                                      


Quiz: We want to estimate the effect from years of education (x1) on the income (y). We determined the following formula for 1.000 people with Å· = 500 + 300*x1. What is the correct answer?

[1]: With ten years of education you exactly earn 3500 monetary units.
[2]: Within the 1.000 individuals, each additional year in education results in 300 more monetary units.
[3]: We only have 1.000 observations - we cannot make a statement.
[4]: Within the 1.000 individuals, each additional year in education results in an average of 300 more monetary units -> to make statements beyond our sample, we rather need more data and more variables.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Simple_Regression")
```

Obviously, the amount of income depends not only on the years of education (yoe). In reality, many more measurable variables affect the amount of income. For instance, the older people get the more they tend to earn. Hence, the age also effects the amount of income. In following we add the variable age as a so called *control variable* to the regression from the quiz:  

$$ y = x_0 + Î²_1yoe + Î²_2age + \varepsilon $$

Now we directly control for the effect of age on income, so that age no longer affects the coefficient on price. However, adding to many control variables could lead to over fitting, which means that the model is overfitted to the specific sample, so that the model reacts to random variation instead of actual contexts. 


Quiz: What is the correct answer?

[1]: Adding the age to the regression increases the value of Î²1.
[2]: Adding the age to the regression decreases the value of Î²1.
[3]: Adding the age to the regression does not influence the coefficient of yoe.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Multiple_Regression")
```

The ideal number of control variables generally depends on the research question and the number of observations. But even with a high frequently data set like the Amazon data set it would be recommended not to control for too many variables (Kranz, 2022). 

We then perform a multiple regression in R. Regression results are usually presented in the form of tables, listing the values of the coefficients. For illustration, we regress the sales rank on the price from the main Amazon data set:  

$$ y = x_0 + Î²_1Price + Î²_2StarRating + \varepsilon $$

**Task:** Replace the ___ gaps with the correct code. Do not forget to press `check`.

```{r "10_1",warning=FALSE}
# We draw a sample of 100.000 observations for performance purposes from `data`
dataLite <- data[sample(nrow(____), size = _____, replace = TRUE),]
# We use the function lm() to regress `pamzn` and `R` on `rank`
regFirstStep <- __(rank ~ _____ + _, data = dataLite)
# The function summary() creates a regression table
summary(regFirstStep)
```

The first column, starting from row two, contains the estimated coefficients for `pamzn` and `R`. The value of 14052.8 indicates that an increase of `pamzn` by one unit on average results in an increase of the sales rank by 14052.8 positions. In the second column, the standard error is given. This is a calculation to determine the accuracy of the individual estimator. Basically, the lower the standard error is, the less the estimator varies. The formula is as follows and can be found [here](https://bookdown.org/mike/data_analysis/linear-regression.html).

$$\widehat{SE}(\hat\beta_k)=s\sqrt{[(\mathbf{X}^T\mathbf{X})^{-1}]_{kk}}$$

Where $s$ is the standard error of the entire regression, $X$ the the covariance matrix, and $\hat\beta_k$ the regression coefficient. The t-value is the estimate divided by its individual standard error. The last column shows the significance level, which indicates the probability of the estimator being exactly as high as it is merely by chance.


Quiz: What is the correct answer?

[1]: A high level on significance indicates an expressive model.
[2]: The significance level gives no information about the expressiveness of the model.
[3]: To determine the expressiveness, we need a combination of the significance level and other values.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("t_value")
```

The R-squared value in the regression table below indicates a coefficient of determination within $[0;1]$, which provides information on how the explanatory variables fit the respective model. In this context, a perfect R-squared value of 1 means a perfect coefficient of determination. In practice, the R-squared value increases with a higher number of observations and more or better chosen explanatory variables. Adding too many variables to the regression might give the illusion of a higher R-squared value, but it also leads to overfitting. 

#### **Robust Standard Errors**

To apply regressions correctly, certain assumptions must be made. One of these assumptions is **homoskedasticity**. To fulfill this assumption, the residuals of the regression must be uniformly distributed. **Robust Statistics** addresses making estimates that are insensitive to small changes in the basic assumptions like outlier values falsifying the regression residuals (Prof.Dr. Rachev, 2007). In R, there are different methods to implement Robust Standard Errors, all of which follow a similar approach. Basically, this algorithm determines the coefficients by disregarding outlier values and other disruptive factors. 

**Task:** Run the following code to run a regression with robust standard errors. Do not forget to press `check`.

```{r "10_2",warning=FALSE}
# Load the package `fixest` to use the following functions feols()
________(fixest)
# We use the function feols() to add Fixed Effects and Robust Standard Errors. Use `dataLite` for your estimate.
regRSE <- _____(____ ~ pamzn + R, vcov = "hetero", data = ________)
# The function summary() creates a regression table
_______(regRSE)
```

There are almost no differences in the coefficients for `pamzn` and `R`, while there are large differences in the standard errors. If the assumption of homoskedasticity is fulfilled, estimators without robust standard errors tend to have lower standard errors. This could be due to the fact, that Robust Standard Errors take uncertainties into consideration which makes the estimate somewhat less imprecise. However, Robust Standard Errors should still be used to guarantee homoscedasticity. 


Quiz: We assume that (for the same regression) the standard errors for the regular regression are lower than for the regression where robust standard errors were used. Whats is the correct answer?

[1]: You can omit the robust standard errors, as they reduce the accuracy of the coefficients.
[2]: Robust standard errors should generally be added to any regression, as regressions without them are generally inaccurate.
[3]: It is advisable to add robust standard errors but not necessary. Even with higher standard errors, it may be important to avoid heteroscedasticity.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Robust_standard_errors")
```

**Note:** Heteroscedasticity is the opposite of homoskedasticity.

#### **Fixed Effects**

To make sense of so-called fixed effects, we start with an explanation of fixed effects followed by their implementation in R. Before we start explaining fixed effects, we focus on the **endogeneity problem** of explanatory variables. The endogeneity problem occurs when the explanatory variable $x_k$ depends on the error term $\varepsilon $. For instance, we examine the effect of price (as the only explanatory variable) on the sales rank. Since the book price is determined by many other factors, the price appears to be an endogenous variable. As a result, biases arise and the respective coefficient could become inaccurate. When using fixed effects, we select certain variables to control for. These fixed effects are kept constant and do not affect the estimation. Using this method, we attempt to reduce the variation within explanatory variables by minimizing the potential for bias from omitted variables (Hill, Davis, Roos and French, 2020).


Quiz: What is your suggestion, does the implementation of fixed effects significantly affect the value of the coefficients of the explanatory variable(s)?

[1]: Yes.
[2]: No.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Fixed_Effects")
```

Let us look at an example for this. We want to create a regression where the sales rank indicates the dependent variables $y$ and $x_1$  as explanatory variable for the Amazon book price. The formula looks as follows:  

$$ y = Î²_0 + Î²_1 x_1 + \varepsilon $$

In the following, the variable `canum` (chapter 2.1) serves us as fixed effects. The regression considers the variable canum as fixed, controlling for each expression of the variable `canum`. Thus, book- and country-specific differences should eliminate effects on the price.  

**Task:** Replace the ___ gaps to create a regressions with fixed effects. Compare this regression `regFirstStep` with a regression without differences occur. Press `check` after solving this exercise. 

```{r "10_3",warning=FALSE}

# Load the package `modelsummary` to gain a more handsome regression table
library(____________) 
# Create a regression using `canum` as fixed effects. Add robust standard errors. 
regFE <- feols(____ ~ pamzn | _____, ____ = "he____", data = dataLite)
# Create a regular regression without robust standard errors and fixed effects.
regWFE <- lm(rank ~ pamzn, data = dataLite)
modelsummary(list(regWFE, regFE), statistic = "({std.error})", coef_rename = c("pamzn" = "Amazon Price"))
```

Column two shows the fixed-effects estimate with robust standard errors, while column one shows the regular regression. It is noticeable that the regressions estimate completely different values for the coefficients. Chapter 2.3 showed price differences that can be attributed to the respective country in which the book was rated. The regular regression seems to show exactly this endogeneity problem, that the Amazon price also depends on the country where the book was rated, as well as many other factors. Using fixed effects, we eliminated the country-specific effect and obtained the information that the Amazon price and the sales rank are negatively related. 

The fixed-effects model requires a so-called panel data structure that has data available for each individual (here: Books) at different points in time. However, the addition of fixed effects can also lead to inaccuracies and biases.                                        The disadvantage is that the variables used as fixed effects can no longer be included in the regression as explanatory variables. This model also assumes that the explanatory variable is not collinear (perfectly correlated) to the fixed effects, otherwise this explanatory variable would be determined purely by the fixed effects. Finally, the endogeneity problem can not get completely eliminated by a fixed-effects model. Better suited for this purpose is the Difference-in-Difference approach or Instrumental Variable Estimation, which are not further discussed in the course of this problem set.                                                      

**Summary**

To sum up, this chapter has given us a handsome overview about some basics and advanced methods and tools for working with regressions. First, we learned how regressions basically work, how to interpret them using particular values, and how they are implemented in R. We distinguished between linear and multiple regressions and how control variables potentially affect multiple regressions. We found multiple differences in the regression coefficients and the coefficients of determination when we added control variables. Second, the effect of Robust Standard Errors was discussed and implemented in R. We extracted the information that inserting Robust Standard Errors can be quite useful to ensure homoskedasticity. Finally, we learned how to explain and to implement fixed effects. Application on the Amazon data set underscored the utility and potential of fixed effects for subsequent examinations. 

In the following chapter, we apply the methods and tools from this chapter to examine the overall effects on sales ranks and prices.

## Exercise 3.2 -- Estimation of the Effects on Sales Ranks and Prices

After the introduction for regressions, we focus on the research question to estimate the effect from professional reviews and crowd ratings on the book market. Replicating the underlying regressions by Reimers and Waldfogel, the data set must first be transformed.  

**Task:** Create the data set `dataUS` that only includes observations from the US market. To enable this, use the condition `cno == 3`. Finally, use `arrange()` to sort the data by `canum` and then by `ddate`. Press `check` to confirm your solution. 

```{r "11_1",warning=FALSE}
# Enter your code here.
```

In their paper, Reimers and Waldfogel used logarithmic values for their estimation. The logarithmization of dependent or explanatory variables can be associated with many advantages. First, logarithmization can be used to stabilize the variance of the residuals to avoid heteroskedasticity. Further, outlier values are mitigated in their effect so that they have less influence on the magnitude of the regression coefficient. The third advantage is due to interpretation of the regression coefficients. Logarithmized coefficients allow us to interpret these coefficients as percentages for simplicity. The possible interpretation are as follows (Kranz, 2022):

* **log - log**: log $\hat/y$ = $\hat\beta_0$ + log $Î²_1 x_1$, for a one percent increase in $x_1$, the predicted value of $y$ increases by approximately $Î²_1$ percent. 

* **log - level**: log $\hat/y$ = $\hat\beta_0$ + $Î²_1 x_1$, for a one unit increase in $x_1$, the predicted value of $y$ increases by approximately 100 * $Î²_1$ percent.

* **level - log**: $\hat/y$ = $\hat\beta_0$ + log $Î²_1 x_1$, for a one percent increase in $x_1$, the predicted value of $y$ increases by approximately 0.01 * $Î²_1$ units. 


## Exercise 4 -- Summary statistics
a) We often want to compute some summary statistic of a vector.
For example:
```{r "12_a"}
x = 10:20
# Computing the sum of x
sum(x)
```
Now compute the mean of x.
```{r "12_a_2"}
# Enter your code here.
```
## Exercise 5 -- Computing with vectors
a) Let y be a vector that contains the squared elements of x. Show y
```{r "13_a"}
# Enter your code here.
```










